{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import time\n",
    "import cv2\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./test.mp4\"\n",
    "cap = cv2.VideoCapture(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = cap.get(cv2.CAP_PROP_FPS)  # Frames per second\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Width of the frames\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Height of the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_path = 'output_video3.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(path, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 118.6ms\n",
      "Speed: 2.0ms preprocess, 118.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 30.0ms\n",
      "Speed: 2.0ms preprocess, 30.0ms inference, 12.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 35.3ms\n",
      "Speed: 2.0ms preprocess, 35.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 28.1ms\n",
      "Speed: 2.0ms preprocess, 28.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 45.1ms\n",
      "Speed: 0.0ms preprocess, 45.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 46.8ms\n",
      "Speed: 0.0ms preprocess, 46.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 42.8ms\n",
      "Speed: 3.0ms preprocess, 42.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 33.3ms\n",
      "Speed: 0.0ms preprocess, 33.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 35.7ms\n",
      "Speed: 0.0ms preprocess, 35.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 41.0ms\n",
      "Speed: 2.0ms preprocess, 41.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 46.9ms\n",
      "Speed: 0.0ms preprocess, 46.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 36.9ms\n",
      "Speed: 0.0ms preprocess, 36.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 47.8ms\n",
      "Speed: 0.0ms preprocess, 47.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 29.9ms\n",
      "Speed: 2.0ms preprocess, 29.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 42.8ms\n",
      "Speed: 2.0ms preprocess, 42.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 29.1ms\n",
      "Speed: 2.0ms preprocess, 29.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 30.4ms\n",
      "Speed: 2.0ms preprocess, 30.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 41.5ms\n",
      "Speed: 0.0ms preprocess, 41.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 36.3ms\n",
      "Speed: 14.2ms preprocess, 36.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 41.6ms\n",
      "Speed: 0.0ms preprocess, 41.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 49.0ms\n",
      "Speed: 0.0ms preprocess, 49.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 28.8ms\n",
      "Speed: 3.0ms preprocess, 28.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 48.8ms\n",
      "Speed: 0.0ms preprocess, 48.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 46.5ms\n",
      "Speed: 0.0ms preprocess, 46.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 37.0ms\n",
      "Speed: 0.0ms preprocess, 37.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 30.2ms\n",
      "Speed: 2.0ms preprocess, 30.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 41.9ms\n",
      "Speed: 0.0ms preprocess, 41.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 44.4ms\n",
      "Speed: 0.0ms preprocess, 44.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 32.3ms\n",
      "Speed: 2.0ms preprocess, 32.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 37.1ms\n",
      "Speed: 3.0ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 30.8ms\n",
      "Speed: 0.0ms preprocess, 30.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 36.3ms\n",
      "Speed: 0.0ms preprocess, 36.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 30.5ms\n",
      "Speed: 2.0ms preprocess, 30.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 46.9ms\n",
      "Speed: 0.0ms preprocess, 46.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 43.6ms\n",
      "Speed: 0.0ms preprocess, 43.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 38.9ms\n",
      "Speed: 1.8ms preprocess, 38.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 42.1ms\n",
      "Speed: 1.9ms preprocess, 42.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 39.7ms\n",
      "Speed: 2.1ms preprocess, 39.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 39.1ms\n",
      "Speed: 2.0ms preprocess, 39.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 27.5ms\n",
      "Speed: 2.0ms preprocess, 27.5ms inference, 13.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 40.9ms\n",
      "Speed: 2.0ms preprocess, 40.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 49.0ms\n",
      "Speed: 0.0ms preprocess, 49.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 45.1ms\n",
      "Speed: 0.0ms preprocess, 45.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 41.3ms\n",
      "Speed: 3.0ms preprocess, 41.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 38.9ms\n",
      "Speed: 2.0ms preprocess, 38.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 46.5ms\n",
      "Speed: 0.0ms preprocess, 46.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 29.9ms\n",
      "Speed: 3.0ms preprocess, 29.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 46.8ms\n",
      "Speed: 0.0ms preprocess, 46.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "type <class 'str'>\n",
      "type2: <class 'str'>\n",
      "\n",
      "0: 384x640 (no detections), 36.5ms\n",
      "Speed: 0.0ms preprocess, 36.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# Encode the frame as base64\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m _, frame_data \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgray_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m base64_frame \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(frame_data)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Write the annotated frame to the output video\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()  # Read a frame from the video\n",
    "    if not ret:\n",
    "        break  # Exit the loop if there are no more frames\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(frame)  # Run inference on the frame\n",
    "\n",
    "    # Annotate the frame with bounding boxes and labels\n",
    "    annotated_frame = results[0].plot()  # Use the first result for drawing\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Encode the frame as base64\n",
    "    _, frame_data = cv2.imencode(\".jpg\", gray_frame)\n",
    "    base64_frame = base64.b64encode(frame_data).decode(\"utf-8\")\n",
    "    # Write the annotated frame to the output video\n",
    "    print(\"type\",type(base64_frame))\n",
    "    data='string'\n",
    "    print(\"type2:\", type(data))\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    # Display the annotated frame (optional)\n",
    "    # cv2.imshow('YOLOv8 Detection', annotated_frame)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Output video saved as {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mediapipe as mp\n",
    "# from mediapipe.tasks import python\n",
    "# from mediapipe.tasks.python import vision\n",
    "# from modelfile.utils import visualize\n",
    "# base_options = python.BaseOptions(model_asset_path='./modelfile/efficientdet_lite0.tflite')\n",
    "# options = vision.ObjectDetectorOptions(base_options=base_options,\n",
    "#                                        running_mode=vision.RunningMode.LIVE_STREAM,\n",
    "#                                        score_threshold=0.5,\n",
    "#                                        result_callback=visualize)\n",
    "\n",
    "# detector = vision.ObjectDetector.create_from_options(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()  # Read a frame from the video\n",
    "#     if not ret:\n",
    "#         break  # Exit the loop if there are no more frames\n",
    "\n",
    "#     # Perform object detection\n",
    "#     # frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "#     frameindex=cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "#     timestamp= (1000 * frameindex / fps)\n",
    "#     print(timestamp)\n",
    "#     results = detector.detect_async(mp_image,  timestamp)  # Run inference on the frame\n",
    "#     out.write(results)\n",
    "\n",
    "#     # Display the annotated frame (optional)\n",
    "#     # cv2.imshow('YOLOv8 Detection', annotated_frame)\n",
    "\n",
    "#     # Exit if 'q' is pressed\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# out.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# print(f\"Output video saved as {output_video_path}\")\n",
    "\n",
    "# # mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=numpy_frame_from_opencv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
